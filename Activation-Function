import numpy as np
import matplotlib.pyplot as plt

x = np.linspace(-5,5,100)

plt.plot(x, 1 / (1+np.exp(-x)), label='sigmoid')
plt.plot(x, np.tanh(x), label='tanh')
plt.plot(x, np.maximum(x,0), label='ReLU')
plt.plot(x, x, label='Identity')
# The softmax function is typically used for classification layers and requires a vector of values.
# Plotting it on a single input x might not be representative, but I'll include it for completeness.
plt.plot(x, np.exp(x) / np.sum(np.exp(x)), label='softmax')
plt.xlabel('Input')
plt.ylabel('Activation')
plt.title('Common Activation Functions')
plt.legend()
plt.grid(True)
plt.show()
